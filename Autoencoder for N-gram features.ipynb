{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation learning for protein data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyfaidx import Fasta\n",
    "genes = Fasta('data/training.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "proteins = []\n",
    "for a in genes:\n",
    "    proteins.append(str(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of proteins 552884\n"
     ]
    }
   ],
   "source": [
    "print(\"number of proteins\",len(proteins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "amino_acids = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L', 'M', 'N', 'P', 'Q',\n",
    "       'R', 'S', 'T', 'V', 'W', 'Y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate possible combinations of amino acids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "twograms = map(''.join, product(amino_acids, repeat=2))\n",
    "threegrams = map(''.join, product(amino_acids, repeat=3))\n",
    "fourgrams = map(''.join, product(amino_acids, repeat=4))\n",
    "fivegrams = map(''.join, product(amino_acids, repeat=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "twogramsList = {}\n",
    "i = 0\n",
    "for word in twograms:\n",
    "    twogramsList[word] = i\n",
    "    i = i + 1\n",
    "    \n",
    "threegramsList = {}\n",
    "i = 0\n",
    "for word in threegrams:\n",
    "    threegramsList[word] = i\n",
    "    i = i + 1\n",
    "\n",
    "fourgramsList = {}\n",
    "i = 0\n",
    "for word in fourgrams:\n",
    "    fourgramsList[word] = i\n",
    "    i = i + 1\n",
    "\n",
    "fivegramsList = {}\n",
    "i = 0\n",
    "for word in fivegrams:\n",
    "    fivegramsList[word] = i\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "8000\n",
      "160000\n",
      "3200000\n"
     ]
    }
   ],
   "source": [
    "print(len(twogramsList))\n",
    "print(len(threegramsList))\n",
    "print(len(fourgramsList))\n",
    "print(len(fivegramsList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build training data for autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can not build 500k X 3.2m matrix locally. So we will train small autoencoders for small batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Model paramters\n",
    "n = 2\n",
    "feature_batch = 400\n",
    "n_record_batches = 1\n",
    "gramModel = twogramsList\n",
    "numOfRecords = int(100000/n_record_batches)\n",
    "n_feature_batches = int(len(gramModel)/feature_batch)\n",
    "\n",
    "# Autoencoder parameters\n",
    "encoding_dim = 50\n",
    "n_epochs = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams,FreqDist\n",
    "twogramsDF = np.zeros([100000,feature_batch])\n",
    "rowId = 1\n",
    "\n",
    "tempgramList = {k: v for k, v in gramModel.items() if v < feature_batch}\n",
    "\n",
    "for sentence in proteins[1:numOfRecords]:\n",
    "    grams = ngrams(sentence, n)\n",
    "    myFd = FreqDist(grams)\n",
    "    for x in myFd:\n",
    "        a = \"\".join(x)\n",
    "        if a in tempgramList:\n",
    "            twogramsDF[rowId,tempgramList[a]] = myFd[x]/ len(sentence)\n",
    "    rowId = rowId + 1\n",
    "    if rowId % 1000 == 0:\n",
    "        print(rowId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets train the autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 400)\n",
      "\n",
      "\t Autoencoder training started: \n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "80000/80000 [==============================] - 3s - loss: 0.0558 - val_loss: 8.8446e-04\n",
      "Epoch 2/10\n",
      "80000/80000 [==============================] - 4s - loss: 4.7119e-04 - val_loss: 2.7052e-04\n",
      "Epoch 3/10\n",
      "80000/80000 [==============================] - 3s - loss: 2.0007e-04 - val_loss: 1.5160e-04\n",
      "Epoch 4/10\n",
      "80000/80000 [==============================] - 4s - loss: 1.2478e-04 - val_loss: 1.0418e-04\n",
      "Epoch 5/10\n",
      "80000/80000 [==============================] - 4s - loss: 9.0487e-05 - val_loss: 7.9440e-05\n",
      "Epoch 6/10\n",
      "80000/80000 [==============================] - 4s - loss: 7.1273e-05 - val_loss: 6.4529e-05\n",
      "Epoch 7/10\n",
      "80000/80000 [==============================] - 4s - loss: 5.9161e-05 - val_loss: 5.4688e-05\n",
      "Epoch 8/10\n",
      "80000/80000 [==============================] - 4s - loss: 5.0915e-05 - val_loss: 4.7774e-05\n",
      "Epoch 9/10\n",
      "80000/80000 [==============================] - 4s - loss: 4.4988e-05 - val_loss: 4.2689e-05\n",
      "Epoch 10/10\n",
      "80000/80000 [==============================] - 4s - loss: 4.0553e-05 - val_loss: 3.8815e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8364060dd8>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "x_train = twogramsDF\n",
    "\n",
    "print(x_train.shape)\n",
    "decoding_dim = x_train.shape[1]\n",
    "\n",
    "protein_input = Input(shape=(x_train.shape[1],))\n",
    "\n",
    "encoded = Dense(encoding_dim, activation='relu',)(protein_input)\n",
    "decoded = Dense(decoding_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input=protein_input, output=decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss='mean_squared_error')\n",
    "\n",
    "print()\n",
    "print(\"\\t Autoencoder training started: \")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                nb_epoch=n_epochs,\n",
    "                batch_size=50,\n",
    "                shuffle=True,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think mean squared error might be misleading as it is too low. It seems like we are not learning anything. Lets try R2 square error to see if we are actually alearning anything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder with R2 squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def r2_score_(y_true, y_pred):\n",
    "    #print(y_pred)\n",
    "    num = K.mean(K.square(y_pred - y_true), axis=-1) \n",
    "    denom = K.mean(K.square(y_true - K.mean(y_true)), axis=-1) \n",
    "    return (num/denom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 400)\n",
      "\n",
      "\t Autoencoder training started: \n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/10\n",
      "80000/80000 [==============================] - 4s - loss: 345.8333 - mean_squared_error: 0.0042 - val_loss: 0.7893 - val_mean_squared_error: 1.5581e-05\n",
      "Epoch 2/10\n",
      "80000/80000 [==============================] - 4s - loss: 0.7769 - mean_squared_error: 1.5289e-05 - val_loss: 0.7866 - val_mean_squared_error: 1.5514e-05\n",
      "Epoch 3/10\n",
      "80000/80000 [==============================] - 4s - loss: 0.7739 - mean_squared_error: 1.5230e-05 - val_loss: 0.7841 - val_mean_squared_error: 1.5459e-05\n",
      "Epoch 4/10\n",
      "80000/80000 [==============================] - 4s - loss: 0.7702 - mean_squared_error: 1.5158e-05 - val_loss: 0.7811 - val_mean_squared_error: 1.5396e-05\n",
      "Epoch 5/10\n",
      "80000/80000 [==============================] - 4s - loss: 0.7637 - mean_squared_error: 1.5028e-05 - val_loss: 0.7738 - val_mean_squared_error: 1.5260e-05\n",
      "Epoch 6/10\n",
      "80000/80000 [==============================] - 4s - loss: 0.7527 - mean_squared_error: 1.4808e-05 - val_loss: 0.7629 - val_mean_squared_error: 1.5056e-05\n",
      "Epoch 7/10\n",
      "80000/80000 [==============================] - 4s - loss: 0.7416 - mean_squared_error: 1.4619e-05 - val_loss: 0.7537 - val_mean_squared_error: 1.4903e-05\n",
      "Epoch 8/10\n",
      "80000/80000 [==============================] - 4s - loss: 0.7337 - mean_squared_error: 1.4487e-05 - val_loss: 0.7457 - val_mean_squared_error: 1.4775e-05\n",
      "Epoch 9/10\n",
      "80000/80000 [==============================] - 5s - loss: 0.7265 - mean_squared_error: 1.4358e-05 - val_loss: 0.7364 - val_mean_squared_error: 1.4625e-05\n",
      "Epoch 10/10\n",
      "80000/80000 [==============================] - 4s - loss: 0.7187 - mean_squared_error: 1.4206e-05 - val_loss: 0.7258 - val_mean_squared_error: 1.4446e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f835c420da0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "x_train = twogramsDF\n",
    "\n",
    "print(x_train.shape)\n",
    "decoding_dim = x_train.shape[1]\n",
    "\n",
    "protein_input = Input(shape=(x_train.shape[1],))\n",
    "\n",
    "encoded = Dense(encoding_dim, activation='relu',)(protein_input)\n",
    "decoded = Dense(decoding_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input=protein_input, output=decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss=r2_score_,metrics=['mean_squared_error'])\n",
    "\n",
    "print()\n",
    "print(\"\\t Autoencoder training started: \")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                nb_epoch=n_epochs,\n",
    "                batch_size=50,\n",
    "                shuffle=True,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that we are able to reduce r2 error on both training and validation data. Let's increase epochs to see how far we can see the improvements in r2 error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epoch set to 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Overridden\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 400)\n",
      "\n",
      "\t Autoencoder training started: \n",
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/50\n",
      "80000/80000 [==============================] - 4s - loss: 354.6874 - mean_squared_error: 0.0043 - val_loss: 0.7892 - val_mean_squared_error: 1.5582e-05\n",
      "Epoch 2/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.7766 - mean_squared_error: 1.5285e-05 - val_loss: 0.7862 - val_mean_squared_error: 1.5506e-05\n",
      "Epoch 3/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.7732 - mean_squared_error: 1.5218e-05 - val_loss: 0.7837 - val_mean_squared_error: 1.5452e-05\n",
      "Epoch 4/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.7688 - mean_squared_error: 1.5131e-05 - val_loss: 0.7788 - val_mean_squared_error: 1.5351e-05\n",
      "Epoch 5/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.7607 - mean_squared_error: 1.4966e-05 - val_loss: 0.7700 - val_mean_squared_error: 1.5179e-05\n",
      "Epoch 6/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.7489 - mean_squared_error: 1.4732e-05 - val_loss: 0.7586 - val_mean_squared_error: 1.4973e-05\n",
      "Epoch 7/50\n",
      "80000/80000 [==============================] - 5s - loss: 0.7385 - mean_squared_error: 1.4576e-05 - val_loss: 0.7500 - val_mean_squared_error: 1.4837e-05\n",
      "Epoch 8/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.7304 - mean_squared_error: 1.4451e-05 - val_loss: 0.7411 - val_mean_squared_error: 1.4693e-05\n",
      "Epoch 9/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.7224 - mean_squared_error: 1.4299e-05 - val_loss: 0.7298 - val_mean_squared_error: 1.4505e-05\n",
      "Epoch 10/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.7142 - mean_squared_error: 1.4154e-05 - val_loss: 0.7201 - val_mean_squared_error: 1.4347e-05\n",
      "Epoch 11/50\n",
      "80000/80000 [==============================] - 5s - loss: 0.7065 - mean_squared_error: 1.4049e-05 - val_loss: 0.7122 - val_mean_squared_error: 1.4217e-05\n",
      "Epoch 12/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6994 - mean_squared_error: 1.3925e-05 - val_loss: 0.7041 - val_mean_squared_error: 1.4088e-05\n",
      "Epoch 13/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6922 - mean_squared_error: 1.3801e-05 - val_loss: 0.6966 - val_mean_squared_error: 1.3965e-05\n",
      "Epoch 14/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6848 - mean_squared_error: 1.3665e-05 - val_loss: 0.6892 - val_mean_squared_error: 1.3837e-05\n",
      "Epoch 15/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6773 - mean_squared_error: 1.3523e-05 - val_loss: 0.6818 - val_mean_squared_error: 1.3710e-05\n",
      "Epoch 16/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6700 - mean_squared_error: 1.3385e-05 - val_loss: 0.6744 - val_mean_squared_error: 1.3583e-05\n",
      "Epoch 17/50\n",
      "80000/80000 [==============================] - 5s - loss: 0.6632 - mean_squared_error: 1.3251e-05 - val_loss: 0.6684 - val_mean_squared_error: 1.3477e-05\n",
      "Epoch 18/50\n",
      "80000/80000 [==============================] - 5s - loss: 0.6575 - mean_squared_error: 1.3143e-05 - val_loss: 0.6635 - val_mean_squared_error: 1.3388e-05\n",
      "Epoch 19/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6527 - mean_squared_error: 1.3040e-05 - val_loss: 0.6596 - val_mean_squared_error: 1.3317e-05\n",
      "Epoch 20/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6485 - mean_squared_error: 1.2950e-05 - val_loss: 0.6563 - val_mean_squared_error: 1.3254e-05\n",
      "Epoch 21/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6448 - mean_squared_error: 1.2867e-05 - val_loss: 0.6529 - val_mean_squared_error: 1.3190e-05\n",
      "Epoch 22/50\n",
      "80000/80000 [==============================] - 5s - loss: 0.6414 - mean_squared_error: 1.2792e-05 - val_loss: 0.6499 - val_mean_squared_error: 1.3131e-05\n",
      "Epoch 23/50\n",
      "80000/80000 [==============================] - 5s - loss: 0.6381 - mean_squared_error: 1.2715e-05 - val_loss: 0.6476 - val_mean_squared_error: 1.3075e-05\n",
      "Epoch 24/50\n",
      "80000/80000 [==============================] - 5s - loss: 0.6350 - mean_squared_error: 1.2646e-05 - val_loss: 0.6445 - val_mean_squared_error: 1.3011e-05\n",
      "Epoch 25/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6319 - mean_squared_error: 1.2580e-05 - val_loss: 0.6421 - val_mean_squared_error: 1.2959e-05\n",
      "Epoch 26/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6288 - mean_squared_error: 1.2515e-05 - val_loss: 0.6391 - val_mean_squared_error: 1.2900e-05\n",
      "Epoch 27/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6257 - mean_squared_error: 1.2451e-05 - val_loss: 0.6366 - val_mean_squared_error: 1.2861e-05\n",
      "Epoch 28/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6227 - mean_squared_error: 1.2392e-05 - val_loss: 0.6336 - val_mean_squared_error: 1.2808e-05\n",
      "Epoch 29/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6197 - mean_squared_error: 1.2331e-05 - val_loss: 0.6310 - val_mean_squared_error: 1.2767e-05\n",
      "Epoch 30/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6166 - mean_squared_error: 1.2270e-05 - val_loss: 0.6289 - val_mean_squared_error: 1.2732e-05\n",
      "Epoch 31/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6137 - mean_squared_error: 1.2220e-05 - val_loss: 0.6261 - val_mean_squared_error: 1.2691e-05\n",
      "Epoch 32/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6107 - mean_squared_error: 1.2163e-05 - val_loss: 0.6233 - val_mean_squared_error: 1.2646e-05\n",
      "Epoch 33/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.6078 - mean_squared_error: 1.2100e-05 - val_loss: 0.6208 - val_mean_squared_error: 1.2608e-05\n",
      "Epoch 34/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6049 - mean_squared_error: 1.2048e-05 - val_loss: 0.6183 - val_mean_squared_error: 1.2561e-05\n",
      "Epoch 35/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.6019 - mean_squared_error: 1.1988e-05 - val_loss: 0.6153 - val_mean_squared_error: 1.2513e-05\n",
      "Epoch 36/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.5990 - mean_squared_error: 1.1929e-05 - val_loss: 0.6131 - val_mean_squared_error: 1.2468e-05\n",
      "Epoch 37/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.5960 - mean_squared_error: 1.1870e-05 - val_loss: 0.6106 - val_mean_squared_error: 1.2420e-05\n",
      "Epoch 38/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.5930 - mean_squared_error: 1.1804e-05 - val_loss: 0.6078 - val_mean_squared_error: 1.2379e-05\n",
      "Epoch 39/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.5900 - mean_squared_error: 1.1744e-05 - val_loss: 0.6054 - val_mean_squared_error: 1.2332e-05\n",
      "Epoch 40/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.5870 - mean_squared_error: 1.1683e-05 - val_loss: 0.6027 - val_mean_squared_error: 1.2285e-05\n",
      "Epoch 41/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.5839 - mean_squared_error: 1.1613e-05 - val_loss: 0.6003 - val_mean_squared_error: 1.2236e-05\n",
      "Epoch 42/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.5808 - mean_squared_error: 1.1548e-05 - val_loss: 0.5973 - val_mean_squared_error: 1.2187e-05\n",
      "Epoch 43/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.5778 - mean_squared_error: 1.1484e-05 - val_loss: 0.5947 - val_mean_squared_error: 1.2151e-05\n",
      "Epoch 44/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.5747 - mean_squared_error: 1.1424e-05 - val_loss: 0.5924 - val_mean_squared_error: 1.2111e-05\n",
      "Epoch 45/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.5717 - mean_squared_error: 1.1366e-05 - val_loss: 0.5899 - val_mean_squared_error: 1.2072e-05\n",
      "Epoch 46/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.5687 - mean_squared_error: 1.1306e-05 - val_loss: 0.5873 - val_mean_squared_error: 1.2040e-05\n",
      "Epoch 47/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.5658 - mean_squared_error: 1.1250e-05 - val_loss: 0.5849 - val_mean_squared_error: 1.2005e-05\n",
      "Epoch 48/50\n",
      "80000/80000 [==============================] - 4s - loss: 0.5629 - mean_squared_error: 1.1193e-05 - val_loss: 0.5825 - val_mean_squared_error: 1.1970e-05\n",
      "Epoch 49/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.5601 - mean_squared_error: 1.1147e-05 - val_loss: 0.5804 - val_mean_squared_error: 1.1949e-05\n",
      "Epoch 50/50\n",
      "80000/80000 [==============================] - 3s - loss: 0.5573 - mean_squared_error: 1.1098e-05 - val_loss: 0.5780 - val_mean_squared_error: 1.1920e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f835c21bef0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "x_train = twogramsDF\n",
    "\n",
    "print(x_train.shape)\n",
    "decoding_dim = x_train.shape[1]\n",
    "\n",
    "protein_input = Input(shape=(x_train.shape[1],))\n",
    "\n",
    "encoded = Dense(encoding_dim, activation='relu',)(protein_input)\n",
    "decoded = Dense(decoding_dim, activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input=protein_input, output=decoded)\n",
    "\n",
    "autoencoder.compile(optimizer='adadelta', loss=r2_score_,metrics=['mean_squared_error'])\n",
    "\n",
    "print()\n",
    "print(\"\\t Autoencoder training started: \")\n",
    "\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                nb_epoch=n_epochs,\n",
    "                batch_size=50,\n",
    "                shuffle=True,validation_split=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are able to reduce R2 error to almost half. Similary I have tested autoencoders on 3,4,5 gram models. On higher gram models its performance is not good. I will try to use deep auto encoders to see if performance improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
